{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('disc08.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Discussion 8: Multiple Linear Regression and Feature Engineering\n",
    "\n",
    "In this lab, we will work through the process of:\n",
    "1. Implementing a linear model \n",
    "1. Defining loss functions\n",
    "1. Feature engineering\n",
    "1. Minimizing loss functions using numeric libraries and analytical methods \n",
    "\n",
    "This lab will continue using the toy tip calculation dataset used in Lab 7.\n",
    "\n",
    "**This assignment is NOT due at any point in time and does not need to be submitted to OkPy**\n",
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** at the top of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data-text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Loading the Tips Dataset\n",
    "\n",
    "To begin, let's load the tips dataset from the `seaborn` library.  This dataset contains records of tips, total bill, and information about the person who paid the bill. This is the same dataset used in Lab 5, so it should look familiar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data-code",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = sns.load_dataset(\"tips\")\n",
    "\n",
    "print(\"Number of Records:\", len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 1: Defining the Model and Feature Engineering\n",
    "\n",
    "In the previous lab, we defined a simple linear model with only one parameter. Now let's make a more complicated model that utilizes other features in our dataset. Let our prediction for tip be a combination of the following features:\n",
    "\n",
    "$$\n",
    "\\text{Tip} = \\theta_1 \\cdot \\text{total_bill} + \\theta_2 \\cdot \\text{sex} + \\theta_3 \\cdot \\text{smoker} + \\theta_4 \\cdot \\text{day} + \\theta_5 \\cdot \\text{time} + \\theta_6 \\cdot \\text{size}\n",
    "$$\n",
    "\n",
    "Notice that some of these features are not numbers! But our linear model will need to predict a numerical value. Let's start by converting some of these non-numerical values into numerical values. Below we split the tips and the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "split-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tips = data['tip']\n",
    "X = data.drop(columns='tip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 1a: Feature Engineering\n",
    "\n",
    "First, let's convert our features to numerical values. A straightforward approach is to map some of these non-numerical features into numerical ones. \n",
    "\n",
    "For example, we can treat the day as a value from 1-7. However, one of the disadvantages in directly translating to a numeric value is that we unintentionally assign certain features disproportionate weight. Consider assigning Sunday to the numeric value of 7, and Monday to the numeric value of 1. In our linear model, Sunday will have 7 times the influence of Monday, which can lower the accuracy of our model.\n",
    "\n",
    "Instead, let's use one-hot encoding to better represent these features! \n",
    "\n",
    "As discussed in lecture, one-hot encoding will produce a binary vector indicating the non-numeric feature. Sunday would be encoded as a [0 0 0 0 0 0 1]. This assigns a more even weight across each category in non-numeric features. Complete the code below to one-hot encode our dataset, allowing us to see the transformed dataset named `one_hot_X`. This dataframe holds our \"featurized\" data, which is also often denoted by $\\phi$.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded dataframe of our input data.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: a dataframe that may include non-numerical features\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded dataframe that only contains numeric features\n",
    "    \n",
    "    Hint: Check out the pd.get_dummies function\n",
    "    \"\"\"\n",
    "    ...\n",
    "one_hot_X = one_hot_encode(X)\n",
    "one_hot_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 1b: Defining the Model\n",
    "\n",
    "Now that all of our data is numeric, we can begin to define our model function. Notice that after one-hot encoding our data, we now have 12 features instead of 6. Therefore, our linear model now looks like:\n",
    "\n",
    "$$\n",
    "\\text{Tip} = \\theta_1 \\cdot \\text{size} + \\theta_2 \\cdot \\text{total_bill} + \\theta_3 \\cdot \\text{day_Thur} + \\theta_4 \\cdot \\text{day_Fri} + ... + \\theta_{11} \\cdot \\text{time_Lunch} + \\theta_{12} \\cdot \\text{time_Dinner}\n",
    "$$\n",
    "\n",
    "We can represent the linear combination above as a matrix-vector product. Implement the `linear_model` function to evaluate this product.\n",
    "\n",
    "**Hint**: You can use [np.dot](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.dot.html), [pd.DataFrame.dot](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dot.html), or the `@` operator to multiply matrices/vectors. However, while the `@` operator can be used to multiply `numpy` arrays, it generally will not work between two `pandas` objects, so keep that in mind when computing matrix-vector products!\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(thetas, X):\n",
    "    \"\"\"\n",
    "    Return the linear combination of thetas and features as defined above.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    thetas: a 1D vector representing the parameters of our model ([theta1, theta2, ...])\n",
    "    X: a 2D dataframe of numeric features\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A 1D vector representing the linear combination of thetas and features as defined above.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 2: Fitting the Model using Numeric Methods\n",
    "\n",
    "Recall in the previous lab we defined multiple loss functions and found optimal theta using the scipy.minimize function. Adapt the loss functions and optimization code from the previous lab (provided below) to work with our new linear model.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def l1(y, y_hat):\n",
    "    return np.abs(y - y_hat)\n",
    "\n",
    "def l2(y, y_hat):\n",
    "    return (y - y_hat)**2\n",
    "\n",
    "def minimize_average_loss(loss_function, model, X, y):\n",
    "    \"\"\"\n",
    "    Minimize the average loss calculated from using different theta vectors, and \n",
    "    estimate the optimal theta for the model.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    loss_function: either the squared or absolute loss functions defined above\n",
    "    model: the model (as defined in Question 1b)\n",
    "    X: a 2D dataframe of numeric features (one-hot encoded)\n",
    "    y: a 1D vector of tip amounts\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    The estimate for the optimal theta vector that minimizes our loss\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Notes on the following function call which you need to finish:\n",
    "    # \n",
    "    # 0. The first '...' should be replaced with the average loss evaluated on \n",
    "    #       the data X, y using the model and appropriate loss function.\n",
    "    # 1. x0 are the initial values for THETA.  Yes, this is confusing\n",
    "    #       but optimization people like x to be the thing they are \n",
    "    #       optimizing. Replace the second '...' with an initial value for theta,\n",
    "    #       and remember that theta is now a vector. DO NOT hard-code the length of x0;\n",
    "    #       it should depend on the number of features in X.\n",
    "    # 2. Your answer will be very similar to your answer to question 2 from lab 5.\n",
    "    ...\n",
    "    return minimize(lambda theta: ..., x0=...)['x']\n",
    "    # Notice above that we extract the 'x' entry in the dictionary returned by `minimize`. \n",
    "    # This entry corresponds to the optimal theta estimated by the function.\n",
    "\n",
    "minimize_average_loss(l2, linear_model, one_hot_X, tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Question 3: Fitting the Model using Analytic Methods\n",
    "\n",
    "Let's also fit our model analytically, for the l2 loss function. In this question we will derive an analytical solution, fit our model and compare our results with our numerical optimization results.\n",
    "\n",
    "Recall that if we're fitting a linear model with the l2 loss function, we are performing least squares! Remember, we are solving the following optimization problem for least squares:\n",
    "\n",
    "$$\\min_{\\theta} ||X\\theta - y||^2$$\n",
    "\n",
    "Let's begin by deriving the analytic solution to least squares. We begin by expanding out the l2 norm and multiplying out all of the terms.\n",
    "\n",
    "<table style=\"width:75%\">\n",
    "  <tr>\n",
    "    <th style=\"text-align: center\">Math</th>\n",
    "    <th style=\"text-align: center\">Explanation</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$||X\\theta - y||^2 = (X\\theta - y)^T (X\\theta - y)$$</td>\n",
    "    <td>Expand l2 norm using the definition for matrices.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$ = (\\theta^T X^T - y^T) (X\\theta - y)$$</td>\n",
    "    <td>Distribute the transpose operator. Remember that $(AB)^T = B^TA^T$.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$ = \\theta^T X^T X \\theta - \\theta^T X^T y - y^T X \\theta + y^T y$$</td>\n",
    "    <td>Multiply out all of the terms (FOIL).</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$ = \\theta^T X^T X \\theta - 2\\theta^T X^T y + y^T y$$</td>\n",
    "    <td>The two middle terms are both transposes of each other, and they are both scalars (since we have a 1xn row vector times an nxn matrix times an nx1 column vector). Since the transpose of a scalar is still the same scalar, we can combine the two middle terms.</td> \n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Whew! Now that we have everything expanded out and somewhat simplified, let's take the gradient of the expression above and set it to the zero vector. This will allow us to solve for the optimal $\\theta$ that will minimize our loss.\n",
    "\n",
    "<table style=\"width:75%\">\n",
    "  <tr>\n",
    "    <th style=\"text-align: center\">Math</th>\n",
    "    <th style=\"text-align: center\">Explanation</th> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$\\nabla_\\theta (\\theta^T X^T X \\theta) - \\nabla_\\theta(2\\theta^TX^T y) + \\nabla_\\theta(y^T y) = \\vec{0}$$</td>\n",
    "    <td>Let's take derivatives one term at a time.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$(X^T X + (X^T X)^T)\\theta - \\nabla_\\theta(2\\theta^TX^T y) + \\nabla_\\theta(y^T y) = \\vec{0}$$</td>\n",
    "    <td>For the first term, we use the identity $\\frac{\\partial}{\\partial x} x^T A x = (A + A^T)x$.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$(X^T X + (X^T X)^T)\\theta - 2X^T y + \\nabla_\\theta(y^T y) = \\vec{0}$$</td>\n",
    "    <td>For the second term, we use the identity $\\frac{\\partial}{\\partial x} x^T A = A$.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$(X^T X + (X^T X)^T)\\theta - 2X^T y + \\vec{0} = \\vec{0}$$</td>\n",
    "    <td>The last derivative is the easiest, since $y^T y$ does not depend on $\\theta$.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$2X^T X\\theta = 2X^T y$$</td>\n",
    "    <td>Notice that $(X^T X)^T = X^T X$, so we can combine the $X^T X$ terms into $2X^TX$. We also move $2X^Ty$ to the right side of the equation.</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>$$\\theta = (X^T X)^{-1} X^T y$$</td>\n",
    "    <td>Divide by 2 on both sides, then left-multiply by $(X^T X)^{-1}$ on both sides to solve for $\\theta$.</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Question 3a: Solving for Theta\n",
    "\n",
    "Now that we have the analytic solution for $\\theta$, let's find the optimal numerical thetas for our tips dataset. Fill out the function below. \n",
    "\n",
    "Hints:\n",
    "1. Use the `np.linalg.inv` function to compute matrix inverses\n",
    "1. To compute the transpose of a matrix, you can use `X.T` or `X.transpose()`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_analytical_sol(X, y):\n",
    "    \"\"\"\n",
    "    Computes the analytical solution to our least squares problem\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: a 2D dataframe of numeric features (one-hot encoded)\n",
    "    y: a 1D vector of tip amounts\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    The estimate for theta computed using the equation mentioned above\n",
    "    \"\"\"\n",
    "    ...\n",
    "analytical_thetas = get_analytical_sol(one_hot_X, tips)\n",
    "print(\"Our analytical loss is: \", l2(linear_model(analytical_thetas, one_hot_X), tips).mean())\n",
    "print(\"Our numerical loss is: \", l2(linear_model(minimize_average_loss(l2, linear_model, one_hot_X, tips), one_hot_X), tips).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q3a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 3b: Fixing our analytical loss\n",
    "\n",
    "Our analytical loss is surprisingly much worse than our numerical loss. Why is this? \n",
    "\n",
    "Here is a relevant Stack Overflow post: https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li\n",
    "\n",
    "In summary, `np.linalg.inv` loses precision, which propogates error throughout the calculation. If you're not convinced, try `np.linalg.solve` instead of `np.linalg.inv`, you'll find that our loss is much closer to the expected numerical loss. These results are meant to demonstrate that even if our math is correct, limits of our computational precision and machinery can lead us to poor results. \n",
    "\n",
    "You might also notice that `one_hot_X` has a rank of 9 while it has 12 columns. This means that $X^TX$ will also have a rank of 9 and 12 columns; thus, $X^TX$ will not be invertible because it does not have full column rank.\n",
    "\n",
    "Complete the code below to one-hot-encode our dataset such that `one_hot_X_revised` has no redundant features. After this, you should see that the analytical loss and the numerical loss are similar as expected.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_revised(data):\n",
    "    \"\"\"\n",
    "    Return the one-hot encoded dataframe of our input data, removing redundancies.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    data: a dataframe that may include non-numerical features\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A one-hot encoded dataframe that only contains numeric features without any redundancies.\n",
    "    \n",
    "    Hint: Check out the drop_first parameter of the pd.get_dummies function.\n",
    "    \"\"\"\n",
    "    ...\n",
    "one_hot_X_revised = one_hot_encode_revised(X)\n",
    "revised_analytical_thetas = get_analytical_sol(one_hot_X_revised, tips)\n",
    "print(\"Our analytical loss is: \", l2(linear_model(revised_analytical_thetas, one_hot_X_revised), tips).mean())\n",
    "print(\"Our numerical loss is: \", l2(linear_model(minimize_average_loss(l2, linear_model, one_hot_X_revised, tips), one_hot_X_revised), tips).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q3b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to submit.\n",
    "ok.submit()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
